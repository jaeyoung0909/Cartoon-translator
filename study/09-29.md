## Machine translation
Attention

ref

[paper](https://arxiv.org/abs/1706.03762)

[platfarm 개발자 blog](https://medium.com/platfarm/%EC%96%B4%ED%85%90%EC%85%98-%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC-transfomer-self-attention-842498fd3225)

[고려대 대학원생 blog](https://blog.naver.com/PostView.nhn?blogId=bcj1210&logNo=221606214423&redirect=Dlog&widgetTypeCall=true&fbclid=IwAR3UKKEQ0w8IN67tDal_qLUOLb2TosXVqVLESJqOkiItDLgf7M7fWtluRkE)

[이미지로 설명하시는 분](http://jalammar.github.io/illustrated-transformer/)

## OCR

Optical character recognition. 딥러닝 이전에도 어느정도 됐고 비전 분야에서 거의 첫 번째 과제였음.

특징 

Text density : 글에는 글자들이 모여있는데 현실에선 아님

Structure of text : 글의 글자들은 행으로 구성되어있는데 현실에는 마구잡이로 나타남

Font, character type, artifacts, location.

현실 세계 데이타셋 : SVHN. 유명하고 간단함. 구글 스트릿 뷰에서 마이닝

# Strategies

- text detection. Dense 한지, sparse 한지에 영향받음

- detection 이후, 그 데이타를 기반으로 다시 세 가지 방법이 있음

  - 전통적 기법

    1. filter 로 글자를 배경으로부터 분리
    2. contour detection 으로 character 하나씩 인식
       - 이 부분이 generalization 하기 힘듦. 많은 manual fine tuning 필요.
    3. 2 에서 얻은걸로 image detection 후 character identify.

  - specialized deep learning

    제일 잘됨.

    1. EAST ( Efficient accurate scene text detector ) : 간단하고 성능좋은 text detection

       text detection 만 함. open CV 에 있음. >v4. 논문에선 PVANet 쓰는데 opencv 에서는 Resnet 씀.

       사실상 U-net

    2. CRNN

       처음에는 그냥 CNN 써서 feature layer 모으고 나중에는 이 레이어를 feature columens로 나눔.

       그리고 이 feature columns 를 bidrectional LSTM 으로 분석해서 character 간의 관계 파악.

       CTC loss 이용. Messy character sequence 를 자연스럽게 

    3. STN-net/SEE ( Semi Supervised End-toEnd Scene Text Recognition )

       완전히 end2end 로 detection & recognition 함.

       bounding box 없이 training => 데이터 더 많이 필요. 두 줄 이상의 text 는 한 번에 처리 

       

  - Standard deep learning

    dnn 에서 detection 에 자주 쓰이는 architecture; SSD, YOLO, Mask RCNN 이용

    개 고양이 디텍션보다 글자 디뎉ㄱ션이 더 안됨.

